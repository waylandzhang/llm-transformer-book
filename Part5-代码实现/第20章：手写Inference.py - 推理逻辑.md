# ç¬¬ 20 ç« ï¼šæ‰‹å†™ Inference.py - æ¨ç†é€»è¾‘

> **ä¸€å¥è¯æ€»ç»“**ï¼šæ¨ç†å°±æ˜¯ï¼šåŠ è½½æ¨¡å‹ â†’ è¾“å…¥ prompt â†’ è‡ªå›å½’ç”Ÿæˆ â†’ è§£ç è¾“å‡ºã€‚ä»£ç åªæœ‰ 30 è¡Œï¼Œä½†è¿™æ˜¯æ¨¡å‹"å¼€å£è¯´è¯"çš„æ—¶åˆ»ã€‚

> ğŸ“¦ **å®Œæ•´ä»£ç ä»“åº“**ï¼š[github.com/waylandzhang/Transformer-from-scratch](https://github.com/waylandzhang/Transformer-from-scratch)

---

## 20.1 æ¨ç† vs è®­ç»ƒ

### 20.1.1 å›é¡¾ç¬¬ 16 ç« çš„å¯¹æ¯”

| | è®­ç»ƒ | æ¨ç† |
|---|-----|------|
| **ç›®çš„** | å­¦ä¹ å‚æ•° | ç”Ÿæˆæ–‡æœ¬ |
| **è¾“å…¥** | å®Œæ•´åºåˆ— + ç›®æ ‡ | åªæœ‰ prompt |
| **è¾“å‡º** | æŸå¤±å€¼ | ç”Ÿæˆçš„æ–‡æœ¬ |
| **å‚æ•°æ›´æ–°** | æ˜¯ | å¦ |
| **Dropout** | å¼€å¯ | å…³é—­ |

### 20.1.2 æ¨ç†çš„æ ¸å¿ƒæµç¨‹

```
1. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
2. å°† prompt ç¼–ç ä¸º token IDs
3. è‡ªå›å½’ç”Ÿæˆï¼ˆä¸€ä¸ªè¯ä¸€ä¸ªè¯ï¼‰
4. è§£ç å›æ–‡æœ¬
```

---

## 20.2 åŠ è½½æ¨¡å‹

### 20.2.1 åŠ è½½æ£€æŸ¥ç‚¹

```python
# åŠ è½½æ¨¡å‹
import torch
import tiktoken
from model import Model

# åŠ è½½æ£€æŸ¥ç‚¹
checkpoint = torch.load('model/model.ckpt')

# ä»æ£€æŸ¥ç‚¹æ¢å¤è¶…å‚æ•°
h_params = checkpoint['h_params']

# é‡å»ºæ¨¡å‹
model = Model(h_params)

# åŠ è½½å‚æ•°
model.load_state_dict(checkpoint['model_state_dict'])

# åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
model.eval()

# ç§»åˆ°æ­£ç¡®çš„è®¾å¤‡
model.to(h_params['device'])
```

### 20.2.2 ä¸ºä»€ä¹ˆéœ€è¦ `model.eval()`ï¼Ÿ

`model.eval()` åšäº†ä¸¤ä»¶äº‹ï¼š
1. **å…³é—­ Dropout**ï¼šæ¨ç†æ—¶ä¸éœ€è¦éšæœºä¸¢å¼ƒ
2. **å›ºå®š BatchNorm**ï¼šä½¿ç”¨è®­ç»ƒæ—¶çš„ç»Ÿè®¡é‡

ä¸åˆ‡æ¢åˆ° eval æ¨¡å¼ï¼Œæ¯æ¬¡æ¨ç†ç»“æœå¯èƒ½ä¸ä¸€æ ·ï¼

---

## 20.3 å‡†å¤‡è¾“å…¥

### 20.3.1 ç¼–ç  prompt

```python
# ç¼–ç è¾“å…¥
encoding = tiktoken.get_encoding("cl100k_base")

# ä½ æƒ³è®©æ¨¡å‹ç»­å†™ä»€ä¹ˆï¼Ÿ
start = "å†œå¤«å±±æ³‰ "

# ç¼–ç ä¸º token IDs
start_ids = encoding.encode(start)
print(f"Prompt: {start}")
print(f"Token IDs: {start_ids}")

# è½¬ä¸º Tensor
x = torch.tensor(start_ids, dtype=torch.long, device=h_params['device'])
x = x.unsqueeze(0)  # å¢åŠ  batch ç»´åº¦ï¼š[seq_len] â†’ [1, seq_len]

print(f"Input shape: {x.shape}")
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
Prompt: å†œå¤«å±±æ³‰
Token IDs: [161, 253, 109, 26288, 239, 103]
Input shape: torch.Size([1, 6])
```

---

## 20.4 ç”Ÿæˆæ–‡æœ¬

### 20.4.1 è°ƒç”¨ç”Ÿæˆå‡½æ•°

```python
# ç”Ÿæˆæ–‡æœ¬
with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
    y = model.generate(
        x,
        max_new_tokens=200,   # æœ€å¤šç”Ÿæˆ 200 ä¸ª token
        temperature=0.5,       # æ¸©åº¦ï¼šè¶Šä½è¶Šç¡®å®š
        top_k=None            # ä¸ä½¿ç”¨ top-k
    )

# è§£ç 
output_text = encoding.decode(y[0].tolist())

print('---------------')
print(output_text)
print('---------------')
```

### 20.4.2 ç”Ÿæˆç»“æœç¤ºä¾‹

```
---------------
å†œå¤«å±±æ³‰ å¤©ç„¶æ°´ 550ml ç“¶è£…
å†œå¤«å±±æ³‰ ä¸œæ–¹æ ‘å¶ èŒ‰è‰èŠ±èŒ¶ 500ml
å†œå¤«å±±æ³‰ NFC æ©™æ± 300ml
å†œå¤«å±±æ³‰ ç»´ä»–å‘½æ°´ æŸ æª¬å‘³ 500ml
---------------
```

æ¨¡å‹å­¦ä¼šäº†ç”Ÿæˆçœ‹èµ·æ¥åƒå•†å“åç§°çš„æ–‡æœ¬ï¼

---

## 20.5 ç”Ÿæˆå‚æ•°è¯¦è§£

### 20.5.1 Temperature

```python
y = model.generate(x, temperature=0.5)
```

Temperature æ§åˆ¶è¾“å‡ºçš„"éšæœºæ€§"ï¼š

| Temperature | æ•ˆæœ | é€‚ç”¨åœºæ™¯ |
|------------|------|---------|
| 0.1-0.3 | éå¸¸ç¡®å®šï¼Œé‡å¤æ€§é«˜ | äº‹å®é—®ç­” |
| 0.5-0.7 | å¹³è¡¡éšæœºå’Œç¡®å®š | é€šç”¨åœºæ™¯ |
| 0.8-1.0 | è¾ƒéšæœºï¼Œå¤šæ ·æ€§é«˜ | åˆ›æ„å†™ä½œ |
| > 1.0 | éå¸¸éšæœºï¼Œå¯èƒ½ä¸è¿è´¯ | å®éªŒç”¨ |

### 20.5.2 Top-K Sampling

```python
y = model.generate(x, top_k=50)
```

åªä»æ¦‚ç‡æœ€é«˜çš„ K ä¸ªè¯ä¸­é‡‡æ ·ï¼š

```
åŸå§‹æ¦‚ç‡åˆ†å¸ƒï¼š
  "å¤©" = 0.3, "çŸ¿" = 0.2, "å†°" = 0.15, ...ï¼ˆ100k ä¸ªè¯ï¼‰

Top-K=3 åï¼š
  "å¤©" = 0.5, "çŸ¿" = 0.33, "å†°" = 0.17
  ï¼ˆé‡æ–°å½’ä¸€åŒ–åˆ°è¿™ 3 ä¸ªè¯ï¼‰
```

**å¥½å¤„**ï¼šé¿å…é‡‡æ ·åˆ°ä½æ¦‚ç‡çš„å¥‡æ€ªè¯ã€‚

### 20.5.3 Max New Tokens

```python
y = model.generate(x, max_new_tokens=200)
```

æ§åˆ¶ç”Ÿæˆé•¿åº¦ï¼š
- å¤ªçŸ­ï¼šå¯èƒ½ç”Ÿæˆä¸å®Œæ•´
- å¤ªé•¿ï¼šæµªè´¹è®¡ç®—ï¼Œå¯èƒ½äº§ç”Ÿé‡å¤

---

## 20.6 æ£€æŸ¥æ¨¡å‹å‚æ•°

### 20.6.1 æ‰“å°å‚æ•°é‡

```python
# ç»Ÿè®¡å‚æ•°é‡
total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"æ¨¡å‹å‚æ•°é‡: {total_params:,}")
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
æ¨¡å‹å‚æ•°é‡: 8,234,560
```

### 20.6.2 æŸ¥çœ‹æ¯å±‚å‚æ•°

```python
# æ‰“å°æ¯å±‚å‚æ•°åå’Œå½¢çŠ¶
for name, param in model.state_dict().items():
    print(f"{name}: {param.shape}")
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
token_embedding_lookup_table.weight: torch.Size([100256, 80])
transformer_blocks.0.ln1.weight: torch.Size([80])
transformer_blocks.0.ln1.bias: torch.Size([80])
transformer_blocks.0.mha.heads.0.Wq.weight: torch.Size([20, 80])
transformer_blocks.0.mha.heads.0.Wk.weight: torch.Size([20, 80])
transformer_blocks.0.mha.heads.0.Wv.weight: torch.Size([20, 80])
...
model_out_linear_layer.weight: torch.Size([100256, 80])
model_out_linear_layer.bias: torch.Size([100256])
```

---

## 20.7 å®Œæ•´ inference.py ä»£ç 

```python
# -*- coding: utf-8 -*-
"""
Sample from a trained model
"""
import torch
import tiktoken
from model import Model

# åŠ è½½æ¨¡å‹å’Œè¶…å‚æ•°
checkpoint = torch.load('model/model.ckpt')
h_params = checkpoint['h_params']
model = Model(h_params)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
model.to(h_params['device'])

# åŠ è½½åˆ†è¯å™¨
encoding = tiktoken.get_encoding("cl100k_base")

# è¾“å…¥ prompt
start = "å†œå¤«å±±æ³‰ "
start_ids = encoding.encode(start)
x = torch.tensor(start_ids, dtype=torch.long, device=h_params['device'])[None, ...]

# ç”Ÿæˆ
with torch.no_grad():
    y = model.generate(x, max_new_tokens=200, temperature=0.5, top_k=None)
    print('---------------')
    print(encoding.decode(y[0].tolist()))
    print('---------------')

# æ‰“å°æ¨¡å‹å‚æ•°é‡
total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Model param size: {total_params:,}")

# æ‰“å°æ¨¡å‹å‚æ•°
for name in model.state_dict().keys():
    print(name, model.state_dict()[name].shape)
```

---

## 20.8 ä¸åŒ Prompt çš„æ•ˆæœ

### 20.8.1 å°è¯•ä¸åŒè¾“å…¥

```python
# å°è¯•ä¸åŒçš„ prompt
prompts = [
    "å†œå¤«å±±æ³‰",
    "å¯å£å¯ä¹",
    "å¥¥åˆ©å¥¥",
    "è’™ç‰›"
]

for prompt in prompts:
    x = torch.tensor(encoding.encode(prompt), dtype=torch.long, device=h_params['device'])[None, ...]
    with torch.no_grad():
        y = model.generate(x, max_new_tokens=50, temperature=0.5)
    print(f"Prompt: {prompt}")
    print(f"Output: {encoding.decode(y[0].tolist())}")
    print("---")
```

### 20.8.2 è§‚å¯Ÿç”Ÿæˆæ•ˆæœ

æ¨¡å‹ä¼šæ ¹æ®è®­ç»ƒæ•°æ®çš„æ¨¡å¼æ¥ç”Ÿæˆï¼š
- å¦‚æœè®­ç»ƒæ•°æ®æ˜¯å•†å“åç§°ï¼Œå®ƒä¼šç”Ÿæˆå•†å“åç§°é£æ ¼çš„æ–‡æœ¬
- å¦‚æœè®­ç»ƒæ•°æ®æ˜¯å°è¯´ï¼Œå®ƒä¼šç”Ÿæˆå°è¯´é£æ ¼çš„æ–‡æœ¬
- å¦‚æœè®­ç»ƒæ•°æ®æ˜¯ä»£ç ï¼Œå®ƒä¼šç”Ÿæˆä»£ç é£æ ¼çš„æ–‡æœ¬

**æ¨¡å‹å­¦åˆ°çš„æ˜¯æ•°æ®ä¸­çš„æ¨¡å¼ï¼Œè€Œä¸æ˜¯"ç†è§£"å†…å®¹ã€‚**

---

## 20.9 è‡ªå›å½’ç”Ÿæˆçš„å¯è§†åŒ–

### 20.9.1 é€æ­¥ç”Ÿæˆè¿‡ç¨‹

```python
# å¯è§†åŒ–ç”Ÿæˆè¿‡ç¨‹
def generate_with_trace(model, x, max_new_tokens=10, temperature=1.0):
    """å¸¦è¿½è¸ªçš„ç”Ÿæˆ"""
    encoding = tiktoken.get_encoding("cl100k_base")

    print(f"åˆå§‹ prompt: {encoding.decode(x[0].tolist())}")
    print("---")

    for i in range(max_new_tokens):
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            logits, _ = model(x[:, -model.context_length:])

        # è·å–æœ€åä½ç½®çš„é¢„æµ‹
        logits = logits[:, -1, :] / temperature
        probs = torch.softmax(logits, dim=-1)

        # è·å– top-5 å€™é€‰
        top5_probs, top5_ids = torch.topk(probs[0], 5)
        print(f"Step {i+1} å€™é€‰:")
        for prob, idx in zip(top5_probs, top5_ids):
            print(f"  '{encoding.decode([idx.item()])}': {prob.item():.3f}")

        # é‡‡æ ·
        idx_next = torch.multinomial(probs, num_samples=1)
        x = torch.cat((x, idx_next), dim=1)

        print(f"  â†’ é€‰æ‹©: '{encoding.decode([idx_next[0].item()])}'")
        print(f"  å½“å‰åºåˆ—: {encoding.decode(x[0].tolist())}")
        print("---")

    return x
```

### 20.9.2 è¾“å‡ºç¤ºä¾‹

```
åˆå§‹ prompt: å†œå¤«å±±æ³‰
---
Step 1 å€™é€‰:
  'å¤©': 0.312
  'çŸ¿': 0.198
  'æœ‰': 0.087
  'çº¯': 0.076
  'æ°´': 0.065
  â†’ é€‰æ‹©: 'å¤©'
  å½“å‰åºåˆ—: å†œå¤«å±±æ³‰å¤©
---
Step 2 å€™é€‰:
  'ç„¶': 0.421
  'å±±': 0.156
  'åœ°': 0.089
  'çš„': 0.067
  'ä¸‹': 0.054
  â†’ é€‰æ‹©: 'ç„¶'
  å½“å‰åºåˆ—: å†œå¤«å±±æ³‰å¤©ç„¶
---
...
```

---

## 20.10 å¸¸è§é—®é¢˜

### 20.10.1 ç”Ÿæˆé‡å¤å†…å®¹

**é—®é¢˜**ï¼šæ¨¡å‹ä¸æ–­é‡å¤ç›¸åŒçš„è¯æˆ–çŸ­è¯­ã€‚

**åŸå› **ï¼š
- Temperature å¤ªä½
- è®­ç»ƒæ•°æ®æœ¬èº«æœ‰é‡å¤
- æ¨¡å‹è¿‡æ‹Ÿåˆ

**è§£å†³**ï¼š
- æé«˜ Temperature
- ä½¿ç”¨ Top-K æˆ– Top-P é‡‡æ ·
- æ·»åŠ  repetition penalty

### 20.10.2 ç”Ÿæˆä¹±ç 

**é—®é¢˜**ï¼šè¾“å‡ºæ˜¯ä¹±ç æˆ–ä¸è¿è´¯çš„æ–‡æœ¬ã€‚

**åŸå› **ï¼š
- æ¨¡å‹è®­ç»ƒä¸è¶³
- prompt ä¸åœ¨è®­ç»ƒåˆ†å¸ƒå†…
- Temperature å¤ªé«˜

**è§£å†³**ï¼š
- è®­ç»ƒæ›´å¤šæ­¥
- ä½¿ç”¨æ›´åˆé€‚çš„ prompt
- é™ä½ Temperature

### 20.10.3 é€Ÿåº¦å¤ªæ…¢

**é—®é¢˜**ï¼šç”Ÿæˆæ¯ä¸ª token éƒ½å¾ˆæ…¢ã€‚

**åŸå› **ï¼š
- æ²¡æœ‰ä½¿ç”¨ GPU
- æ²¡æœ‰ KV Cache
- æ¨¡å‹å¤ªå¤§

**è§£å†³**ï¼š
- ä½¿ç”¨ GPUï¼ˆå¦‚æœæœ‰ï¼‰
- å®ç° KV Cacheï¼ˆç¬¬ 22 ç« ï¼‰
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹

---

## 20.11 æœ¬ç« æ€»ç»“

### 20.11.1 æ¨ç†ä¸‰æ­¥æ›²

```
1. åŠ è½½æ¨¡å‹
   checkpoint = torch.load('model.ckpt')
   model.load_state_dict(checkpoint['model_state_dict'])
   model.eval()

2. ç¼–ç  prompt
   start_ids = encoding.encode(prompt)
   x = torch.tensor(start_ids)[None, ...]

3. ç”Ÿæˆ
   with torch.no_grad():
       y = model.generate(x, max_new_tokens=200)
   output = encoding.decode(y[0].tolist())
```

### 20.11.2 å…³é”®å‚æ•°

| å‚æ•° | ä½œç”¨ | å»ºè®®å€¼ |
|-----|------|-------|
| `max_new_tokens` | æœ€å¤§ç”Ÿæˆé•¿åº¦ | 50-500 |
| `temperature` | éšæœºæ€§æ§åˆ¶ | 0.5-0.8 |
| `top_k` | é™åˆ¶å€™é€‰è¯æ•°é‡ | 50-100 |

### 20.11.3 æ ¸å¿ƒè®¤çŸ¥

> **inference.py åªæœ‰ 30 è¡Œä»£ç ï¼Œä½†å®ƒæ˜¯æˆ‘ä»¬æ•´ä¸ªæ—…ç¨‹çš„ç»ˆç‚¹â€”â€”è®©æ¨¡å‹çœŸæ­£"å¼€å£è¯´è¯"ã€‚åŠ è½½å‚æ•°ã€ç¼–ç  promptã€è‡ªå›å½’ç”Ÿæˆã€è§£ç è¾“å‡ºï¼Œè¿™å°±æ˜¯ GPT æ¨ç†çš„å…¨éƒ¨ã€‚ç†è§£äº†è¿™äº›ï¼Œä½ å°±ç†è§£äº† ChatGPT æ˜¯å¦‚ä½•å›å¤ä½ çš„æ¯ä¸€å¥è¯çš„ã€‚**

---

## æœ¬ç« äº¤ä»˜ç‰©

å­¦å®Œè¿™ä¸€ç« ï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹æ£€æŸ¥ç‚¹
- [ ] ç†è§£ `model.eval()` çš„ä½œç”¨
- [ ] ä½¿ç”¨ä¸åŒçš„ Temperature å’Œ Top-K å‚æ•°
- [ ] ç‹¬ç«‹è¿è¡Œæ¨ç†è„šæœ¬

---

## Part 5 æ€»ç»“

æ­å–œï¼ä½ å·²ç»å®Œæˆäº†**ä»£ç å®ç°**éƒ¨åˆ†ï¼š

| ç« èŠ‚ | å†…å®¹ | ä»£ç é‡ |
|-----|------|--------|
| ç¬¬ 18 ç«  | Model.py - æ¨¡å‹å®šä¹‰ | ~200 è¡Œ |
| ç¬¬ 19 ç«  | Train.py - è®­ç»ƒå¾ªç¯ | ~100 è¡Œ |
| ç¬¬ 20 ç«  | Inference.py - æ¨ç†é€»è¾‘ | ~30 è¡Œ |

**æ€»å…±ä¸åˆ° 400 è¡Œä»£ç **ï¼Œä½ å°±å®ç°äº†ä¸€ä¸ªå®Œæ•´çš„ Transformerï¼

è¿™äº›ä»£ç è™½ç„¶ç®€åŒ–ï¼Œä½†åŒ…å«äº†çœŸæ­£ GPT çš„æ ¸å¿ƒé€»è¾‘ã€‚ç†è§£äº†è¿™äº›ï¼Œä½ å°±èƒ½è¯»æ‡‚ Hugging Face transformersã€LLaMAã€GPT-NeoX ç­‰å¼€æºé¡¹ç›®çš„æºç ã€‚

---

## å®Œæ•´ä»£ç 

Part 5 çš„å®Œæ•´å®ç°å¯åœ¨ GitHub è·å–ï¼š

> ğŸ“¦ **[github.com/waylandzhang/Transformer-from-scratch](https://github.com/waylandzhang/Transformer-from-scratch)**

åŒ…å«ï¼š
- `model.py` - å®Œæ•´æ¨¡å‹å®šä¹‰
- `train.py` - è®­ç»ƒè„šæœ¬
- `inference.py` - æ¨ç†è„šæœ¬
- `step-by-step.ipynb` - é€æ­¥è®²è§£çš„ Jupyter notebook

---

## ä¸‹ä¸€ç« é¢„å‘Š

æˆ‘ä»¬çš„æ¨¡å‹èƒ½å·¥ä½œäº†ï¼Œä½†é€Ÿåº¦ä¸å¤Ÿå¿«ã€‚æ¯ç”Ÿæˆä¸€ä¸ª tokenï¼Œéƒ½è¦é‡æ–°è®¡ç®—æ•´ä¸ªåºåˆ—çš„ Attentionâ€”â€”å¤ªæµªè´¹äº†ï¼

ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬è¿›å…¥ **Part 6ï¼šç”Ÿäº§ä¼˜åŒ–**ï¼Œå­¦ä¹  **Flash Attention** å’Œ **KV Cache**ï¼Œè®©æ¨ç†é€Ÿåº¦æå‡æ•°å€ï¼
