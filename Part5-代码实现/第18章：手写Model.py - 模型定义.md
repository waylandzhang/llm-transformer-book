# ç¬¬ 18 ç« ï¼šæ‰‹å†™ Model.py - æ¨¡å‹å®šä¹‰

> **ä¸€å¥è¯æ€»ç»“**ï¼šæ¨¡å‹ä»£ç å°±æ˜¯æŠŠæˆ‘ä»¬å‰é¢å­¦çš„æ‰€æœ‰ç»„ä»¶ï¼ˆEmbeddingã€Positional Encodingã€Multi-Head Attentionã€FFNã€LayerNormï¼‰ç”¨ PyTorch ä¸²èµ·æ¥ã€‚æ¯ä¸ªç±»å¯¹åº”ä¸€ä¸ªç»„ä»¶ï¼Œæ¯è¡Œä»£ç éƒ½æœ‰å¯¹åº”çš„æ•°å­¦å…¬å¼ã€‚

> ğŸ“¦ **å®Œæ•´ä»£ç ä»“åº“**ï¼š[github.com/waylandzhang/Transformer-from-scratch](https://github.com/waylandzhang/Transformer-from-scratch)

---

## 18.1 å†™ä»£ç ä¹‹å‰ï¼šæ•´ä½“ç»“æ„

### 18.1.1 æˆ‘ä»¬è¦å®ç°ä»€ä¹ˆï¼Ÿ

```
Model (å®Œæ•´æ¨¡å‹)
â”œâ”€â”€ Token Embedding (è¯åµŒå…¥)
â”œâ”€â”€ Positional Encoding (ä½ç½®ç¼–ç )
â”œâ”€â”€ N Ã— TransformerBlock (å¤šä¸ª Transformer å—)
â”‚   â”œâ”€â”€ LayerNorm
â”‚   â”œâ”€â”€ Multi-Head Attention
â”‚   â”œâ”€â”€ LayerNorm
â”‚   â””â”€â”€ Feed Forward Network
â”œâ”€â”€ Final LayerNorm (æœ€åçš„å½’ä¸€åŒ–)
â””â”€â”€ Output Linear (è¾“å‡ºæŠ•å½±åˆ°è¯è¡¨)
```

### 18.1.2 ä»£ç æ–‡ä»¶ç»“æ„

æˆ‘ä»¬æŠŠæ‰€æœ‰æ¨¡å‹ä»£ç æ”¾åœ¨ä¸€ä¸ªæ–‡ä»¶ `model.py` ä¸­ï¼š

```python
# model.py æ•´ä½“ç»“æ„
import math
import torch
import torch.nn as nn
from torch.nn import functional as F

class FeedForwardNetwork(nn.Module):     # FFN å‰é¦ˆç½‘ç»œ
    ...

class Attention(nn.Module):              # å•å¤´æ³¨æ„åŠ›
    ...

class MultiHeadAttention(nn.Module):     # å¤šå¤´æ³¨æ„åŠ›
    ...

class TransformerBlock(nn.Module):       # Transformer å—
    ...

class Model(nn.Module):                  # å®Œæ•´æ¨¡å‹
    ...
```

---

## 18.2 Feed Forward Networkï¼ˆå‰é¦ˆç½‘ç»œï¼‰

### 18.2.1 å›é¡¾ FFN ç»“æ„

åœ¨ç¬¬ 15 ç« æˆ‘ä»¬å­¦è¿‡ï¼ŒFFN æ˜¯ä¸€ä¸ªç®€å•çš„ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œï¼š

```
è¾“å…¥ [batch, seq, d_model]
     â†“
Linear1: d_model â†’ d_model Ã— 4
     â†“
ReLU æ¿€æ´»
     â†“
Linear2: d_model Ã— 4 â†’ d_model
     â†“
Dropout
     â†“
è¾“å‡º [batch, seq, d_model]
```

**ç»´åº¦å…ˆæ‰©å¤§ 4 å€ï¼Œå†ç¼©å›æ¥**ã€‚è¿™æ˜¯è®©æ¨¡å‹æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ã€‚

### 18.2.2 ä»£ç å®ç°

```python
# å®šä¹‰å‰é¦ˆç½‘ç»œ Feed Forward Network
class FeedForwardNetwork(nn.Module):
    def __init__(self, d_model, dropout):
        super().__init__()
        self.d_model = d_model
        self.dropout = dropout
        self.ffn = nn.Sequential(
            nn.Linear(self.d_model, self.d_model * 4),  # æ‰©å±• 4 å€
            nn.ReLU(),                                   # æ¿€æ´»å‡½æ•°
            nn.Linear(self.d_model * 4, self.d_model),  # ç¼©å›åŸç»´åº¦
            nn.Dropout(self.dropout)                     # éšæœºä¸¢å¼ƒ
        )

    def forward(self, x):
        return self.ffn(x)
```

### 18.2.3 ä»£ç è§£è¯»

| ä»£ç  | ä½œç”¨ | ç»´åº¦å˜åŒ– |
|------|------|---------|
| `nn.Linear(d_model, d_model * 4)` | ç¬¬ä¸€å±‚å…¨è¿æ¥ | `[B,T,512] â†’ [B,T,2048]` |
| `nn.ReLU()` | æ¿€æ´»å‡½æ•°ï¼Œå¼•å…¥éçº¿æ€§ | ä¸å˜ |
| `nn.Linear(d_model * 4, d_model)` | ç¬¬äºŒå±‚å…¨è¿æ¥ | `[B,T,2048] â†’ [B,T,512]` |
| `nn.Dropout(dropout)` | éšæœºä¸¢å¼ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ | ä¸å˜ |

---

## 18.3 Attentionï¼ˆå•å¤´æ³¨æ„åŠ›ï¼‰

### 18.3.1 å›é¡¾ Attention å…¬å¼

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

åœ¨ä»£ç é‡Œï¼Œæˆ‘ä»¬éœ€è¦å®ç°ï¼š
1. ç”Ÿæˆ Qã€Kã€Vï¼ˆé€šè¿‡çº¿æ€§å˜æ¢ï¼‰
2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼ˆQ @ K^Tï¼‰
3. ç¼©æ”¾ï¼ˆé™¤ä»¥ âˆšd_kï¼‰
4. åº”ç”¨ Causal Maskï¼ˆé˜²æ­¢çœ‹åˆ°æœªæ¥ï¼‰
5. Softmax å½’ä¸€åŒ–
6. ä¸ V ç›¸ä¹˜å¾—åˆ°è¾“å‡º

### 18.3.2 ä»£ç å®ç°

```python
# å®šä¹‰å•å¤´æ³¨æ„åŠ› Scaled Dot Product Attention
class Attention(nn.Module):
    def __init__(self, d_model, head_size, context_length, dropout):
        super().__init__()
        self.d_model = d_model
        self.head_size = head_size
        self.context_length = context_length
        self.dropout = dropout

        # Qã€Kã€V çš„çº¿æ€§å˜æ¢
        self.Wq = nn.Linear(self.d_model, self.head_size, bias=False)
        self.Wk = nn.Linear(self.d_model, self.head_size, bias=False)
        self.Wv = nn.Linear(self.d_model, self.head_size, bias=False)

        # Causal Maskï¼šä¸‹ä¸‰è§’çŸ©é˜µ
        self.register_buffer('mask', torch.tril(torch.ones(self.context_length, self.context_length)))

        self.dropout = nn.Dropout(self.dropout)

    def forward(self, x):
        B, T, C = x.shape  # Batch, Time(seq_len), Channels(d_model)

        # 1. ç”Ÿæˆ Q, K, V
        q = self.Wq(x)  # [B, T, head_size]
        k = self.Wk(x)  # [B, T, head_size]
        v = self.Wv(x)  # [B, T, head_size]

        # 2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° Q @ K^Tï¼Œå¹¶ç¼©æ”¾
        weights = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_size)
        # weights: [B, T, T]

        # 3. åº”ç”¨ Causal Maskï¼ˆæœªæ¥ä½ç½®è®¾ä¸º -infï¼‰
        weights = weights.masked_fill(self.mask[:T, :T] == 0, float('-inf'))

        # 4. Softmax å½’ä¸€åŒ–
        weights = F.softmax(weights, dim=-1)

        # 5. Dropout
        weights = self.dropout(weights)

        # 6. ä¸ V ç›¸ä¹˜
        output = weights @ v  # [B, T, head_size]

        return output
```

### 18.3.3 å…³é”®ä»£ç è§£è¯»

**Causal Mask çš„ä½œç”¨**ï¼š

```python
self.register_buffer('mask', torch.tril(torch.ones(context_length, context_length)))
```

`torch.tril` ç”Ÿæˆä¸‹ä¸‰è§’çŸ©é˜µï¼š
```
[[1, 0, 0, 0],
 [1, 1, 0, 0],
 [1, 1, 1, 0],
 [1, 1, 1, 1]]
```

ä½ç½® i åªèƒ½çœ‹åˆ°ä½ç½® 0 åˆ° iï¼Œä¸èƒ½çœ‹åˆ° i+1 åŠä¹‹åã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå« **Causal**ï¼ˆå› æœï¼‰Maskã€‚

**ä¸ºä»€ä¹ˆç”¨ `register_buffer`ï¼Ÿ**

Mask ä¸æ˜¯å‚æ•°ï¼ˆä¸éœ€è¦è®­ç»ƒï¼‰ï¼Œä½†éœ€è¦è·Ÿæ¨¡å‹ä¸€èµ·ç§»åŠ¨åˆ° GPUã€‚`register_buffer` å°±æ˜¯ä¸“é—¨å¹²è¿™ä¸ªçš„ã€‚

---

## 18.4 Multi-Head Attentionï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰

### 18.4.1 å¤šå¤´çš„æ€è·¯

å¤šå¤´æ³¨æ„åŠ› = **å¤šä¸ªå•å¤´æ³¨æ„åŠ›å¹¶è¡Œè¿è¡Œï¼Œæœ€åæ‹¼æ¥**ã€‚

æ¯ä¸ªå¤´å…³æ³¨ä¸åŒçš„"è§†è§’"ï¼Œæœ€ååˆèµ·æ¥å¾—åˆ°æ›´ä¸°å¯Œçš„è¡¨ç¤ºã€‚

### 18.4.2 ä»£ç å®ç°

```python
# å®šä¹‰å¤šå¤´æ³¨æ„åŠ› Multi-Head Attention
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, head_size, context_length, dropout):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_size = head_size
        self.context_length = context_length
        self.dropout = dropout

        # åˆ›å»ºå¤šä¸ªæ³¨æ„åŠ›å¤´
        self.heads = nn.ModuleList([
            Attention(self.d_model, self.head_size, self.context_length, self.dropout)
            for _ in range(self.num_heads)
        ])

        # è¾“å‡ºæŠ•å½±å±‚ Wo
        self.projection_layer = nn.Linear(self.d_model, self.d_model)
        self.dropout = nn.Dropout(self.dropout)

    def forward(self, x):
        # å¹¶è¡Œè¿è¡Œæ‰€æœ‰å¤´
        head_outputs = [head(x) for head in self.heads]

        # æ‹¼æ¥æ‰€æœ‰å¤´çš„è¾“å‡º
        head_outputs = torch.cat(head_outputs, dim=-1)  # [B, T, num_heads * head_size] = [B, T, d_model]

        # é€šè¿‡è¾“å‡ºæŠ•å½±å±‚
        out = self.dropout(self.projection_layer(head_outputs))

        return out
```

### 18.4.3 ç»´åº¦è¿½è¸ª

å‡è®¾ `d_model=512, num_heads=8, head_size=64`ï¼š

```
è¾“å…¥ x: [B, T, 512]
     â†“
æ¯ä¸ªå¤´è¾“å‡º: [B, T, 64]  # 8 ä¸ªå¤´
     â†“
æ‹¼æ¥: [B, T, 512]  # 64 Ã— 8 = 512
     â†“
Wo æŠ•å½±: [B, T, 512]
     â†“
è¾“å‡º: [B, T, 512]
```

**å…³é”®å…¬å¼**ï¼š`head_size = d_model // num_heads`

---

## 18.5 è®ºæ–‡åŸç‰ˆ Multi-Head Attention

### 18.5.1 è®ºæ–‡å®ç° vs æˆ‘ä»¬çš„å®ç°

ä¸Šé¢çš„å®ç°æ˜¯**ç‰©ç†ä¸Šåˆ†å¼€**çš„ï¼šæ¯ä¸ªå¤´æœ‰ç‹¬ç«‹çš„ Wqã€Wkã€Wvã€‚

è®ºæ–‡ã€ŠAttention is All You Needã€‹çš„å®ç°æ˜¯**é€»è¾‘ä¸Šåˆ†å¼€**çš„ï¼šå…ˆç”¨ä¸€ä¸ªå¤§çš„çº¿æ€§å±‚ï¼Œå† reshape æˆå¤šä¸ªå¤´ã€‚

### 18.5.2 è®ºæ–‡ç‰ˆä»£ç 

```python
# è®ºæ–‡ç‰ˆ Multi-Head Attentionï¼ˆé€»è¾‘åˆ‡åˆ†ï¼‰
class MultiHeadAttention_Paper(nn.Module):
    def __init__(self, d_model, num_heads, head_size, context_length, dropout):
        super().__init__()
        self.context_length = context_length
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_size = head_size

        # ä¸€ä¸ªå¤§çš„çº¿æ€§å±‚ï¼Œè¾“å‡ºç»´åº¦è¿˜æ˜¯ d_model
        self.Wq = nn.Linear(d_model, d_model)
        self.Wk = nn.Linear(d_model, d_model)
        self.Wv = nn.Linear(d_model, d_model)
        self.Wo = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.tril(torch.ones(self.context_length, self.context_length)))

    def split_heads(self, x):
        """é€»è¾‘ä¸Šåˆ‡åˆ†å¤šå¤´"""
        batch_size = x.shape[0]
        context_length = x.shape[1]
        # [B, T, d_model] â†’ [B, T, num_heads, head_size] â†’ [B, num_heads, T, head_size]
        x = x.reshape(batch_size, context_length, self.num_heads, self.head_size)
        x = x.permute(0, 2, 1, 3)
        return x

    def forward(self, x):
        B, T, C = x.shape

        # çº¿æ€§å˜æ¢ååˆ‡åˆ†å¤´
        q = self.split_heads(self.Wq(x))  # [B, num_heads, T, head_size]
        k = self.split_heads(self.Wk(x))
        v = self.split_heads(self.Wv(x))

        # è®¡ç®—æ³¨æ„åŠ›
        weights = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_size)
        weights = weights.masked_fill(self.mask[:T, :T] == 0, float('-inf'))
        weights = F.softmax(weights, dim=-1)
        weights = self.dropout(weights)

        output = weights @ v  # [B, num_heads, T, head_size]

        # åˆå¹¶å¤´ï¼š[B, num_heads, T, head_size] â†’ [B, T, d_model]
        output = output.transpose(1, 2).reshape(-1, T, C)

        # è¾“å‡ºæŠ•å½±
        output = self.Wo(output)

        return output
```

### 18.5.3 ä¸¤ç§å®ç°çš„å¯¹æ¯”

| | ç‰©ç†åˆ†å¼€ | é€»è¾‘åˆ†å¼€ï¼ˆè®ºæ–‡ç‰ˆï¼‰ |
|---|---------|-----------------|
| **Wq/Wk/Wv æ•°é‡** | num_heads ä¸ª | å„ 1 ä¸ª |
| **å‚æ•°é‡** | ç›¸åŒ | ç›¸åŒ |
| **è®¡ç®—æ•ˆç‡** | ç¨ä½ï¼ˆå¾ªç¯ï¼‰ | æ›´é«˜ï¼ˆå¹¶è¡Œï¼‰ |
| **ä»£ç æ¸…æ™°åº¦** | æ›´æ¸…æ™° | ç¨å¤æ‚ |

**å‚æ•°é‡ç›¸åŒ**çš„åŸå› ï¼š
- ç‰©ç†åˆ†å¼€ï¼š`num_heads Ã— (d_model Ã— head_size) = d_model Ã— d_model`
- é€»è¾‘åˆ†å¼€ï¼š`d_model Ã— d_model`

å®é™…ä½¿ç”¨ä¸­ï¼Œ**è®ºæ–‡ç‰ˆæ•ˆç‡æ›´é«˜**ï¼Œå› ä¸ºå¯ä»¥åˆ©ç”¨ GPU å¹¶è¡Œè®¡ç®—ã€‚

---

## 18.6 Transformer Block

### 18.6.1 Block ç»“æ„

<img src="./images/attention-block-step.png" alt="Transformer Block" width="60%" />

æ¯ä¸ª Block åŒ…å«ï¼š
1. LayerNorm â†’ Multi-Head Attention â†’ æ®‹å·®è¿æ¥
2. LayerNorm â†’ FFN â†’ æ®‹å·®è¿æ¥

è¿™æ˜¯ **Pre-Norm** ç»“æ„ï¼ˆGPT-2 ä½¿ç”¨ï¼‰ã€‚

### 18.6.2 ä»£ç å®ç°

```python
# å®šä¹‰ Transformer Block
class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, head_size, context_length, dropout):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        self.mha = MultiHeadAttention(d_model, num_heads, head_size, context_length, dropout)
        self.ffn = FeedForwardNetwork(d_model, dropout)

    def forward(self, x):
        # Attention + æ®‹å·®
        x = x + self.mha(self.ln1(x))
        # FFN + æ®‹å·®
        x = x + self.ffn(self.ln2(x))
        return x
```

### 18.6.3 Pre-Norm vs Post-Norm

**Pre-Norm**ï¼ˆæˆ‘ä»¬ç”¨çš„ï¼‰ï¼š
```python
x = x + self.mha(self.ln1(x))  # å…ˆ Normï¼Œå† Attention
```

**Post-Norm**ï¼ˆåŸå§‹ Transformerï¼‰ï¼š
```python
x = self.ln1(x + self.mha(x))  # å…ˆ Attentionï¼Œå† Norm
```

Pre-Norm è®­ç»ƒæ›´ç¨³å®šï¼Œç°ä»£æ¨¡å‹ï¼ˆGPT-2ã€LLaMAï¼‰éƒ½ç”¨ Pre-Normã€‚

---

## 18.7 å®Œæ•´ Model ç±»

### 18.7.1 Model æ•´ä½“ç»“æ„

```python
# å®šä¹‰å®Œæ•´æ¨¡å‹
class Model(nn.Module):
    def __init__(self, h_params):
        super().__init__()
        # ä»è¶…å‚æ•°å­—å…¸è¯»å–é…ç½®
        self.context_length = h_params['context_length']
        self.d_model = h_params['d_model']
        self.num_blocks = h_params['num_blocks']
        self.num_heads = h_params['num_heads']
        self.head_size = self.d_model // self.num_heads
        self.dropout = h_params['dropout']
        self.device = h_params['device']
        self.max_token_value = h_params['max_token_value']

        # Token Embedding
        self.token_embedding_lookup_table = nn.Embedding(self.max_token_value, self.d_model)

        # Transformer Blocks + æœ€åçš„ LayerNorm
        self.transformer_blocks = nn.Sequential(*(
            [TransformerBlock(self.d_model, self.num_heads, self.head_size,
                              self.context_length, self.dropout)
             for _ in range(self.num_blocks)] +
            [nn.LayerNorm(self.d_model)]
        ))

        # è¾“å‡ºæŠ•å½±å±‚
        self.model_out_linear_layer = nn.Linear(self.d_model, self.max_token_value)
```

### 18.7.2 å‰å‘ä¼ æ’­

```python
def forward(self, idx, targets=None):
    B, T = idx.shape

    # 1. ä½ç½®ç¼–ç ï¼ˆä½¿ç”¨æ­£å¼¦/ä½™å¼¦ï¼‰
    position_encoding_lookup_table = torch.zeros(self.context_length, self.d_model, device=self.device)
    position = torch.arange(0, self.context_length, dtype=torch.float, device=self.device).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float, device=self.device) * (-math.log(10000.0) / self.d_model))
    position_encoding_lookup_table[:, 0::2] = torch.sin(position * div_term)
    position_encoding_lookup_table[:, 1::2] = torch.cos(position * div_term)
    position_embedding = position_encoding_lookup_table[:T, :].to(self.device)

    # 2. Token Embedding + Position Encoding
    x = self.token_embedding_lookup_table(idx) + position_embedding

    # 3. é€šè¿‡æ‰€æœ‰ Transformer Blocks
    x = self.transformer_blocks(x)

    # 4. è¾“å‡ºæŠ•å½±åˆ°è¯è¡¨
    logits = self.model_out_linear_layer(x)

    # 5. å¦‚æœæœ‰ç›®æ ‡ï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰ï¼Œè®¡ç®—æŸå¤±
    if targets is not None:
        B, T, C = logits.shape
        logits_reshaped = logits.view(B * T, C)
        targets_reshaped = targets.view(B * T)
        loss = F.cross_entropy(input=logits_reshaped, target=targets_reshaped)
    else:
        loss = None

    return logits, loss
```

### 18.7.3 å…³é”®ä»£ç è§£è¯»

**ä½ç½®ç¼–ç å…¬å¼**ï¼š

```python
div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * (-math.log(10000.0) / self.d_model))
position_encoding_lookup_table[:, 0::2] = torch.sin(position * div_term)
position_encoding_lookup_table[:, 1::2] = torch.cos(position * div_term)
```

è¿™å°±æ˜¯æˆ‘ä»¬åœ¨ç¬¬ 5 ç« å­¦è¿‡çš„æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç ï¼š
- å¶æ•°ç»´åº¦ç”¨ sin
- å¥‡æ•°ç»´åº¦ç”¨ cos
- é¢‘ç‡éšç»´åº¦å¢åŠ è€Œé™ä½

**æŸå¤±å‡½æ•°**ï¼š

```python
loss = F.cross_entropy(input=logits_reshaped, target=targets_reshaped)
```

äº¤å‰ç†µæŸå¤±ï¼Œè¡¡é‡æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒçš„å·®è·ã€‚

---

## 18.8 ç”Ÿæˆå‡½æ•°

### 18.8.1 è‡ªå›å½’ç”Ÿæˆ

```python
def generate(self, idx, max_new_tokens=100, temperature=1.0, top_k=None):
    """
    è‡ªå›å½’ç”Ÿæˆæ–‡æœ¬

    Args:
        idx: åˆå§‹ token IDs [B, T]
        max_new_tokens: æœ€å¤šç”Ÿæˆå¤šå°‘ä¸ªæ–° token
        temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§
        top_k: åªä» top-k ä¸ªæœ€é«˜æ¦‚ç‡çš„è¯ä¸­é‡‡æ ·
    """
    for _ in range(max_new_tokens):
        # 1. æˆªæ–­åˆ°æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
        idx_crop = idx[:, -self.context_length:]

        # 2. å‰å‘ä¼ æ’­
        logits, loss = self.forward(idx_crop)

        # 3. åªå–æœ€åä¸€ä¸ªä½ç½®çš„ logits
        logits = logits[:, -1, :] / temperature

        # 4. å¯é€‰ï¼šåªä¿ç•™ top-k ä¸ªé€‰é¡¹
        if top_k is not None:
            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
            logits[logits < v[:, [-1]]] = -float('Inf')

        # 5. Softmax å¾—åˆ°æ¦‚ç‡
        probs = F.softmax(input=logits, dim=-1)

        # 6. é‡‡æ ·
        idx_next = torch.multinomial(input=probs, num_samples=1)

        # 7. æ‹¼æ¥åˆ°åºåˆ—
        idx = torch.cat((idx, idx_next), dim=1)

    return idx
```

### 18.8.2 Temperature çš„ä½œç”¨

Temperature åœ¨ç¬¬ 6 ç« è®¨è®ºè¿‡ï¼š

```python
logits = logits[:, -1, :] / temperature
```

- **T < 1**ï¼šæ¦‚ç‡æ›´é›†ä¸­ï¼ˆæ›´ç¡®å®šï¼‰
- **T = 1**ï¼šåŸå§‹æ¦‚ç‡
- **T > 1**ï¼šæ¦‚ç‡æ›´åˆ†æ•£ï¼ˆæ›´éšæœºï¼‰

### 18.8.3 Top-K Sampling

```python
if top_k is not None:
    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
    logits[logits < v[:, [-1]]] = -float('Inf')
```

åªä»æ¦‚ç‡æœ€é«˜çš„ k ä¸ªè¯ä¸­é‡‡æ ·ï¼Œé¿å…ç”Ÿæˆä½æ¦‚ç‡çš„å¥‡æ€ªè¯ã€‚

---

## 18.9 å®Œæ•´ model.py ä»£ç 

```python
"""
Transformer Decoder-only base model for text generation
"""
import math
import torch
import torch.nn as nn
from torch.nn import functional as F


class FeedForwardNetwork(nn.Module):
    def __init__(self, d_model, dropout):
        super().__init__()
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.ReLU(),
            nn.Linear(d_model * 4, d_model),
            nn.Dropout(dropout)
        )

    def forward(self, x):
        return self.ffn(x)


class Attention(nn.Module):
    def __init__(self, d_model, head_size, context_length, dropout):
        super().__init__()
        self.head_size = head_size
        self.Wq = nn.Linear(d_model, head_size, bias=False)
        self.Wk = nn.Linear(d_model, head_size, bias=False)
        self.Wv = nn.Linear(d_model, head_size, bias=False)
        self.register_buffer('mask', torch.tril(torch.ones(context_length, context_length)))
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        B, T, C = x.shape
        q = self.Wq(x)
        k = self.Wk(x)
        v = self.Wv(x)
        weights = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_size)
        weights = weights.masked_fill(self.mask[:T, :T] == 0, float('-inf'))
        weights = F.softmax(weights, dim=-1)
        weights = self.dropout(weights)
        return weights @ v


class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, head_size, context_length, dropout):
        super().__init__()
        self.heads = nn.ModuleList([
            Attention(d_model, head_size, context_length, dropout)
            for _ in range(num_heads)
        ])
        self.projection_layer = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        head_outputs = torch.cat([head(x) for head in self.heads], dim=-1)
        return self.dropout(self.projection_layer(head_outputs))


class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, head_size, context_length, dropout):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        self.mha = MultiHeadAttention(d_model, num_heads, head_size, context_length, dropout)
        self.ffn = FeedForwardNetwork(d_model, dropout)

    def forward(self, x):
        x = x + self.mha(self.ln1(x))
        x = x + self.ffn(self.ln2(x))
        return x


class Model(nn.Module):
    def __init__(self, h_params):
        super().__init__()
        self.context_length = h_params['context_length']
        self.d_model = h_params['d_model']
        self.num_blocks = h_params['num_blocks']
        self.num_heads = h_params['num_heads']
        self.head_size = self.d_model // self.num_heads
        self.dropout = h_params['dropout']
        self.device = h_params['device']
        self.max_token_value = h_params['max_token_value']

        self.token_embedding_lookup_table = nn.Embedding(self.max_token_value, self.d_model)
        self.transformer_blocks = nn.Sequential(*(
            [TransformerBlock(self.d_model, self.num_heads, self.head_size,
                              self.context_length, self.dropout)
             for _ in range(self.num_blocks)] +
            [nn.LayerNorm(self.d_model)]
        ))
        self.model_out_linear_layer = nn.Linear(self.d_model, self.max_token_value)

    def forward(self, idx, targets=None):
        B, T = idx.shape
        # Positional Encoding
        position_encoding = torch.zeros(self.context_length, self.d_model, device=self.device)
        position = torch.arange(0, self.context_length, dtype=torch.float, device=self.device).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float, device=self.device) * (-math.log(10000.0) / self.d_model))
        position_encoding[:, 0::2] = torch.sin(position * div_term)
        position_encoding[:, 1::2] = torch.cos(position * div_term)

        x = self.token_embedding_lookup_table(idx) + position_encoding[:T, :].to(self.device)
        x = self.transformer_blocks(x)
        logits = self.model_out_linear_layer(x)

        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))
        else:
            loss = None
        return logits, loss

    def generate(self, idx, max_new_tokens=100, temperature=1.0, top_k=None):
        for _ in range(max_new_tokens):
            idx_crop = idx[:, -self.context_length:]
            logits, _ = self.forward(idx_crop)
            logits = logits[:, -1, :] / temperature
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float('Inf')
            probs = F.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, idx_next), dim=1)
        return idx
```

---

## 18.10 æœ¬ç« æ€»ç»“

### 18.10.1 ä»£ç ä¸æ¦‚å¿µå¯¹åº”

| ç±» | å¯¹åº”æ¦‚å¿µ | ç« èŠ‚ |
|---|---------|------|
| `FeedForwardNetwork` | å‰é¦ˆç½‘ç»œ | ç¬¬ 7 ç«  |
| `Attention` | å•å¤´æ³¨æ„åŠ› | ç¬¬ 9-12 ç«  |
| `MultiHeadAttention` | å¤šå¤´æ³¨æ„åŠ› | ç¬¬ 11 ç«  |
| `TransformerBlock` | Transformer å— | ç¬¬ 13 ç«  |
| `Model` | å®Œæ•´æ¨¡å‹ | ç¬¬ 15 ç«  |

### 18.10.2 å‚æ•°é‡ä¼°ç®—

å‡è®¾ `d_model=512, num_heads=8, num_blocks=6, vocab_size=50000`ï¼š

| ç»„ä»¶ | å…¬å¼ | å‚æ•°é‡ |
|------|------|--------|
| Token Embedding | vocab Ã— d_model | 2560ä¸‡ |
| Attention (Ã—6) | 4 Ã— d_modelÂ² Ã— 6 | 629ä¸‡ |
| FFN (Ã—6) | 2 Ã— d_model Ã— 4Ã—d_model Ã— 6 | 1258ä¸‡ |
| Output Linear | d_model Ã— vocab | 2560ä¸‡ |

**æ€»è®¡ï¼šçº¦ 7000 ä¸‡å‚æ•°**

### 18.10.3 æ ¸å¿ƒè®¤çŸ¥

> **model.py å°±æ˜¯æŠŠæˆ‘ä»¬å­¦è¿‡çš„æ‰€æœ‰ç»„ä»¶ç”¨ PyTorch ä»£ç ä¸²èµ·æ¥ã€‚æ¯ä¸ªç±»å¯¹åº”ä¸€ä¸ªç»„ä»¶ï¼šFFNã€Attentionã€MultiHeadAttentionã€TransformerBlockã€Modelã€‚ç†è§£äº†è¿™äº›ç»„ä»¶ï¼Œä»£ç å°±æ˜¯è‡ªç„¶è€Œç„¶çš„äº‹æƒ…ã€‚**

---

## æœ¬ç« äº¤ä»˜ç‰©

å­¦å®Œè¿™ä¸€ç« ï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] ç‹¬ç«‹å®ç° FeedForwardNetwork ç±»
- [ ] ç‹¬ç«‹å®ç° Attention ç±»ï¼ˆåŒ…æ‹¬ Causal Maskï¼‰
- [ ] ç‹¬ç«‹å®ç° MultiHeadAttention ç±»
- [ ] ç†è§£è®ºæ–‡ç‰ˆ MHA å’Œç‰©ç†åˆ†å¼€ç‰ˆçš„åŒºåˆ«
- [ ] ç†è§£å®Œæ•´ Model ç±»çš„å‰å‘ä¼ æ’­æµç¨‹

---

## å®Œæ•´ä»£ç 

æœ¬ç« ä»£ç å¯¹åº”çš„å®Œæ•´å®ç°å¯åœ¨ GitHub è·å–ï¼š

> ğŸ“¦ **[github.com/waylandzhang/Transformer-from-scratch](https://github.com/waylandzhang/Transformer-from-scratch)**

åŒ…å« `model.py`ã€`train.py`ã€`inference.py` ä»¥åŠ step-by-step Jupyter notebookã€‚

---

## ä¸‹ä¸€ç« é¢„å‘Š

æ¨¡å‹å®šä¹‰å¥½äº†ï¼Œä½†å®ƒè¿˜ä¸ä¼š"æ€è€ƒ"â€”â€”å‚æ•°éƒ½æ˜¯éšæœºåˆå§‹åŒ–çš„ã€‚

ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬æ¥å†™ **train.py**ï¼Œå®ç°è®­ç»ƒå¾ªç¯ï¼šå‡†å¤‡æ•°æ®ã€è®¡ç®—æŸå¤±ã€åå‘ä¼ æ’­ã€æ›´æ–°å‚æ•°ã€‚è®©æ¨¡å‹çœŸæ­£"å­¦ä¼š"é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼
