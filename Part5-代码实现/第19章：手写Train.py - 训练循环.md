# ç¬¬ 19 ç« ï¼šæ‰‹å†™ Train.py - è®­ç»ƒå¾ªç¯

> **ä¸€å¥è¯æ€»ç»“**ï¼šè®­ç»ƒå¾ªç¯å°±æ˜¯ï¼šå‡†å¤‡æ•°æ® â†’ å‰å‘ä¼ æ’­ â†’ è®¡ç®—æŸå¤± â†’ åå‘ä¼ æ’­ â†’ æ›´æ–°å‚æ•°ï¼Œä¸æ–­é‡å¤ã€‚ä»£ç ä¸åˆ° 100 è¡Œï¼Œä½†è®©æ¨¡å‹ä»"ä¸€æ— æ‰€çŸ¥"å˜æˆèƒ½é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚

> ğŸ“¦ **å®Œæ•´ä»£ç ä»“åº“**ï¼š[github.com/waylandzhang/Transformer-from-scratch](https://github.com/waylandzhang/Transformer-from-scratch)

---

## 19.1 è®­ç»ƒçš„æœ¬è´¨

### 19.1.1 æ¨¡å‹åˆå§‹æ—¶æ˜¯ä»€ä¹ˆçŠ¶æ€ï¼Ÿ

åˆšåˆ›å»ºçš„æ¨¡å‹ï¼Œæ‰€æœ‰å‚æ•°éƒ½æ˜¯**éšæœºåˆå§‹åŒ–**çš„ã€‚è®©å®ƒé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œè¾“å‡ºåŸºæœ¬æ˜¯ä¹±çŒœã€‚

```python
# éšæœºåˆå§‹åŒ–çš„æ¨¡å‹
model = Model(h_params)

# è¾“å…¥ "å†œå¤«å±±æ³‰"
input_ids = tokenizer.encode("å†œå¤«å±±æ³‰")

# æ¨¡å‹è¾“å‡ºï¼šå¯èƒ½æ˜¯ä»»ä½•ä¹±ä¸ƒå…«ç³Ÿçš„å­—ç¬¦
output = model.generate(input_ids)
# å¯èƒ½è¾“å‡ºï¼š"å†œå¤«å±±æ³‰ç¡è§‰æœˆäº®é£æœºæ±½è½¦..."  # å®Œå…¨éšæœº
```

### 19.1.2 è®­ç»ƒçš„ç›®æ ‡

é€šè¿‡å¤§é‡çš„"è¾“å…¥-ç›®æ ‡"å¯¹ï¼Œè®©æ¨¡å‹**å­¦ä¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯**ã€‚

```
è¾“å…¥ï¼šå†œ å¤« å±± æ³‰
ç›®æ ‡ï¼šå¤« å±± æ³‰ å¤©

æ¨¡å‹éœ€è¦å­¦ä¼šï¼š
- çœ‹åˆ°"å†œ"ï¼Œé¢„æµ‹"å¤«"
- çœ‹åˆ°"å†œå¤«"ï¼Œé¢„æµ‹"å±±"
- çœ‹åˆ°"å†œå¤«å±±"ï¼Œé¢„æµ‹"æ³‰"
- çœ‹åˆ°"å†œå¤«å±±æ³‰"ï¼Œé¢„æµ‹"å¤©"ï¼ˆæˆ–å…¶ä»–åˆç†çš„ç»­å†™ï¼‰
```

### 19.1.3 è®­ç»ƒå¾ªç¯çš„å››æ­¥

```
1. å‰å‘ä¼ æ’­ï¼šè¾“å…¥æ•°æ®ï¼Œå¾—åˆ°é¢„æµ‹
2. è®¡ç®—æŸå¤±ï¼šé¢„æµ‹ vs ç›®æ ‡ï¼Œå·®å¤šå°‘ï¼Ÿ
3. åå‘ä¼ æ’­ï¼šæŸå¤±å¯¹æ¯ä¸ªå‚æ•°æ±‚æ¢¯åº¦
4. æ›´æ–°å‚æ•°ï¼šæœç€å‡å°‘æŸå¤±çš„æ–¹å‘è°ƒæ•´
```

é‡å¤è¿™å››æ­¥ï¼ŒæŸå¤±ä¼šé€æ¸ä¸‹é™ï¼Œæ¨¡å‹é¢„æµ‹è¶Šæ¥è¶Šå‡†ã€‚

---

## 19.2 è¶…å‚æ•°é…ç½®

### 19.2.1 è¶…å‚æ•°å­—å…¸

```python
# è¶…å‚æ•°é…ç½®
h_params = {
    # æ¨¡å‹æ¶æ„
    "d_model": 80,           # åµŒå…¥ç»´åº¦ï¼ˆå°æ¨¡å‹ç”¨å°å€¼ï¼‰
    "num_blocks": 6,         # Transformer å—æ•°é‡
    "num_heads": 4,          # æ³¨æ„åŠ›å¤´æ•°

    # è®­ç»ƒé…ç½®
    "batch_size": 2,         # æ¯æ¬¡è®­ç»ƒå¤šå°‘ä¸ªæ ·æœ¬
    "context_length": 128,   # ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆåºåˆ—é•¿åº¦ï¼‰
    "max_iters": 500,        # è®­ç»ƒå¤šå°‘æ­¥
    "learning_rate": 1e-3,   # å­¦ä¹ ç‡

    # æ­£åˆ™åŒ–
    "dropout": 0.1,          # Dropout æ¦‚ç‡

    # è¯„ä¼°é…ç½®
    "eval_interval": 50,     # æ¯å¤šå°‘æ­¥è¯„ä¼°ä¸€æ¬¡
    "eval_iters": 10,        # è¯„ä¼°æ—¶ç”¨å¤šå°‘ä¸ª batch

    # è®¾å¤‡
    "device": "cuda" if torch.cuda.is_available() else "cpu",

    # éšæœºç§å­ï¼ˆå¯å¤ç°ï¼‰
    "TORCH_SEED": 1337
}
```

### 19.2.2 å…³é”®è¶…å‚æ•°è§£é‡Š

| è¶…å‚æ•° | ä½œç”¨ | å…¸å‹å€¼ |
|-------|------|--------|
| `batch_size` | æ¯æ¬¡è®­ç»ƒçš„æ ·æœ¬æ•° | 2-32ï¼ˆå–å†³äºæ˜¾å­˜ï¼‰ |
| `context_length` | æ¨¡å‹èƒ½"çœ‹åˆ°"å¤šé•¿çš„ä¸Šä¸‹æ–‡ | 128-2048 |
| `learning_rate` | å‚æ•°æ›´æ–°çš„æ­¥é•¿ | 1e-3 åˆ° 1e-5 |
| `max_iters` | æ€»å…±è®­ç»ƒå¤šå°‘æ­¥ | æ•°ç™¾åˆ°æ•°ç™¾ä¸‡ |
| `dropout` | éšæœºä¸¢å¼ƒçš„æ¯”ä¾‹ | 0.1-0.3 |

---

## 19.3 æ•°æ®å‡†å¤‡

### 19.3.1 åŠ è½½åŸå§‹æ–‡æœ¬

```python
# åŠ è½½è®­ç»ƒæ•°æ®
with open('data/è®¢å•å•†å“åç§°.csv', 'r', encoding="utf-8") as file:
    text = file.read()

print(f"æ–‡æœ¬é•¿åº¦ï¼š{len(text):,} å­—ç¬¦")
# è¾“å‡ºï¼šæ–‡æœ¬é•¿åº¦ï¼š324,523 å­—ç¬¦
```

### 19.3.2 Tokenization

```python
# ä½¿ç”¨ TikToken åˆ†è¯
import tiktoken

tokenizer = tiktoken.get_encoding("cl100k_base")
tokenized_text = tokenizer.encode(text)

print(f"Token æ•°é‡ï¼š{len(tokenized_text):,}")
# è¾“å‡ºï¼šToken æ•°é‡ï¼š77,919
```

### 19.3.3 è½¬ä¸º Tensor å¹¶åˆ†å‰²æ•°æ®é›†

```python
# è½¬æ¢ä¸º PyTorch Tensor
tokenized_text = torch.tensor(tokenized_text, dtype=torch.long, device=h_params['device'])

# 90% è®­ç»ƒï¼Œ10% éªŒè¯
train_size = int(len(tokenized_text) * 0.9)
train_data = tokenized_text[:train_size]
val_data = tokenized_text[train_size:]

print(f"è®­ç»ƒé›†ï¼š{len(train_data):,} tokens")
print(f"éªŒè¯é›†ï¼š{len(val_data):,} tokens")
```

### 19.3.4 è·å– Batch

```python
# éšæœºè·å–ä¸€ä¸ª batch
def get_batch(split: str):
    """
    è·å–ä¸€ä¸ª batch çš„è®­ç»ƒæ•°æ®

    Args:
        split: 'train' æˆ– 'valid'

    Returns:
        x: è¾“å…¥ [batch_size, context_length]
        y: ç›®æ ‡ [batch_size, context_length]ï¼ˆå³ç§»ä¸€ä½ï¼‰
    """
    data = train_data if split == 'train' else val_data

    # éšæœºé€‰æ‹©èµ·å§‹ä½ç½®
    idxs = torch.randint(
        low=0,
        high=len(data) - h_params['context_length'],
        size=(h_params['batch_size'],)
    )

    # æ„å»ºè¾“å…¥å’Œç›®æ ‡
    x = torch.stack([data[idx:idx + h_params['context_length']] for idx in idxs])
    y = torch.stack([data[idx + 1:idx + h_params['context_length'] + 1] for idx in idxs])

    return x.to(h_params['device']), y.to(h_params['device'])
```

### 19.3.5 ç†è§£ x å’Œ y çš„å…³ç³»

```
å‡è®¾ context_length = 8

åŸå§‹æ•°æ®ï¼š[å†œ, å¤«, å±±, æ³‰, å¤©, ç„¶, æ°´, ç”œ, èœ‚, èœœ, ...]
              â†“
xï¼ˆè¾“å…¥ï¼‰ï¼š[å†œ, å¤«, å±±, æ³‰, å¤©, ç„¶, æ°´, ç”œ]
yï¼ˆç›®æ ‡ï¼‰ï¼š[å¤«, å±±, æ³‰, å¤©, ç„¶, æ°´, ç”œ, èœ‚]

y å°±æ˜¯ x å³ç§»ä¸€ä½ã€‚æ¨¡å‹éœ€è¦å­¦ä¼šï¼šx[i] â†’ y[i]
```

---

## 19.4 æŸå¤±å‡½æ•°

### 19.4.1 äº¤å‰ç†µæŸå¤±

æ¨¡å‹è¾“å‡ºçš„æ˜¯æ¯ä¸ªä½ç½®å¯¹è¯è¡¨ä¸­æ¯ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚æˆ‘ä»¬ç”¨**äº¤å‰ç†µæŸå¤±**æ¥è¡¡é‡é¢„æµ‹å’ŒçœŸå®çš„å·®è·ã€‚

```python
# è®¡ç®—æŸå¤±
loss = F.cross_entropy(
    input=logits_reshaped,    # æ¨¡å‹é¢„æµ‹ [batch*seq, vocab_size]
    target=targets_reshaped   # çœŸå®ç›®æ ‡ [batch*seq]
)
```

### 19.4.2 æŸå¤±è¶Šä½è¶Šå¥½

- **éšæœºåˆå§‹åŒ–**ï¼šæŸå¤±çº¦ 10-11ï¼ˆæ¥è¿‘ ln(vocab_size)ï¼‰
- **è®­ç»ƒå**ï¼šæŸå¤±å¯ä»¥é™åˆ° 2-4
- **è¿‡æ‹Ÿåˆ**ï¼šè®­ç»ƒæŸå¤±å¾ˆä½ï¼ŒéªŒè¯æŸå¤±å¾ˆé«˜

---

## 19.5 è¯„ä¼°å‡½æ•°

### 19.5.1 ä¸ºä»€ä¹ˆéœ€è¦è¯„ä¼°ï¼Ÿ

è®­ç»ƒæŸå¤±ä¸‹é™ä¸ä»£è¡¨æ¨¡å‹çœŸçš„å­¦å¥½äº†â€”â€”å¯èƒ½åªæ˜¯"èƒŒç­”æ¡ˆ"ï¼ˆè¿‡æ‹Ÿåˆï¼‰ã€‚

æˆ‘ä»¬éœ€è¦åœ¨**éªŒè¯é›†**ä¸Šè¯„ä¼°ï¼Œçœ‹æ¨¡å‹å¯¹æ²¡è§è¿‡çš„æ•°æ®è¡¨ç°å¦‚ä½•ã€‚

### 19.5.2 è¯„ä¼°ä»£ç 

```python
# è¯„ä¼°å‡½æ•°
@torch.no_grad()  # ä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜
def estimate_loss():
    out = {}
    model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­ Dropoutï¼‰

    for split in ['train', 'valid']:
        losses = torch.zeros(h_params['eval_iters'])

        for k in range(h_params['eval_iters']):
            x_batch, y_batch = get_batch(split)
            logits, loss = model(x_batch, y_batch)
            losses[k] = loss.item()

        out[split] = losses.mean()

    model.train()  # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
    return out
```

### 19.5.3 `model.train()` vs `model.eval()`

| æ¨¡å¼ | Dropout | BatchNorm |
|------|---------|-----------|
| `model.train()` | éšæœºä¸¢å¼ƒ | ä½¿ç”¨ batch ç»Ÿè®¡é‡ |
| `model.eval()` | ä¸ä¸¢å¼ƒ | ä½¿ç”¨å…¨å±€ç»Ÿè®¡é‡ |

è¯„ä¼°æ—¶å¿…é¡»ç”¨ `model.eval()`ï¼Œå¦åˆ™ç»“æœä¼šæœ‰éšæœºæ€§ã€‚

---

## 19.6 ä¼˜åŒ–å™¨

### 19.6.1 AdamW ä¼˜åŒ–å™¨

```python
# åˆ›å»ºä¼˜åŒ–å™¨
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=h_params['learning_rate']
)
```

AdamW æ˜¯ç›®å‰æœ€å¸¸ç”¨çš„ä¼˜åŒ–å™¨ï¼Œç»“åˆäº†ï¼š
- **Momentum**ï¼šè€ƒè™‘å†å²æ¢¯åº¦æ–¹å‘
- **è‡ªé€‚åº”å­¦ä¹ ç‡**ï¼šæ¯ä¸ªå‚æ•°æœ‰è‡ªå·±çš„å­¦ä¹ ç‡
- **Weight Decay**ï¼šL2 æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

### 19.6.2 ä¸ºä»€ä¹ˆé€‰ AdamWï¼Ÿ

| ä¼˜åŒ–å™¨ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|-------|------|------|
| SGD | ç®€å•ï¼Œæ³›åŒ–å¥½ | æ”¶æ•›æ…¢ |
| Adam | æ”¶æ•›å¿« | å¯èƒ½æ³›åŒ–ä¸å¥½ |
| **AdamW** | æ”¶æ•›å¿« + æ³›åŒ–å¥½ | ç•¥å¤æ‚ |

ç°ä»£å¤§æ¨¡å‹è®­ç»ƒå‡ ä¹éƒ½ç”¨ AdamWã€‚

---

## 19.7 è®­ç»ƒå¾ªç¯

### 19.7.1 å®Œæ•´è®­ç»ƒå¾ªç¯

```python
# è®­ç»ƒå¾ªç¯
for step in range(h_params['max_iters']):

    # å®šæœŸè¯„ä¼°
    if step % h_params['eval_interval'] == 0 or step == h_params['max_iters'] - 1:
        losses = estimate_loss()
        print(f'Step: {step}, '
              f'Training Loss: {losses["train"]:.3f}, '
              f'Validation Loss: {losses["valid"]:.3f}')

    # 1. è·å–ä¸€ä¸ª batch
    xb, yb = get_batch('train')

    # 2. å‰å‘ä¼ æ’­
    logits, loss = model(xb, yb)

    # 3. åå‘ä¼ æ’­
    optimizer.zero_grad(set_to_none=True)  # æ¸…é›¶æ¢¯åº¦
    loss.backward()                         # è®¡ç®—æ¢¯åº¦

    # 4. æ›´æ–°å‚æ•°
    optimizer.step()
```

### 19.7.2 æ¯ä¸€æ­¥è¯¦è§£

**`optimizer.zero_grad()`**ï¼šæ¸…é™¤ä¸Šä¸€æ­¥çš„æ¢¯åº¦ã€‚

PyTorch é»˜è®¤ä¼š**ç´¯åŠ **æ¢¯åº¦ï¼Œæ‰€ä»¥æ¯æ­¥å¼€å§‹å‰è¦æ¸…é›¶ã€‚

**`loss.backward()`**ï¼šåå‘ä¼ æ’­ï¼Œè®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ã€‚

è¿™æ˜¯ PyTorch è‡ªåŠ¨å¾®åˆ†çš„é­”æ³•â€”â€”å®ƒä¼šè‡ªåŠ¨è¿½è¸ªæ‰€æœ‰è®¡ç®—ï¼Œç„¶åæ±‚å¯¼ã€‚

**`optimizer.step()`**ï¼šæ ¹æ®æ¢¯åº¦æ›´æ–°å‚æ•°ã€‚

```
å‚æ•°_new = å‚æ•°_old - learning_rate Ã— æ¢¯åº¦
```

---

## 19.8 è®­ç»ƒè¾“å‡ºç¤ºä¾‹

```
Step: 0, Training Loss: 10.847, Validation Loss: 10.852
Step: 50, Training Loss: 7.234, Validation Loss: 7.198
Step: 100, Training Loss: 5.421, Validation Loss: 5.456
Step: 150, Training Loss: 4.312, Validation Loss: 4.387
Step: 200, Training Loss: 3.876, Validation Loss: 3.921
Step: 250, Training Loss: 3.542, Validation Loss: 3.678
Step: 300, Training Loss: 3.298, Validation Loss: 3.512
Step: 350, Training Loss: 3.112, Validation Loss: 3.398
Step: 400, Training Loss: 2.987, Validation Loss: 3.287
Step: 450, Training Loss: 2.876, Validation Loss: 3.198
Step: 499, Training Loss: 2.798, Validation Loss: 3.145
```

å¯ä»¥çœ‹åˆ°ï¼š
- æŸå¤±ä» ~10.8 ä¸‹é™åˆ° ~2.8
- éªŒè¯æŸå¤±ç•¥é«˜äºè®­ç»ƒæŸå¤±ï¼ˆæ­£å¸¸ï¼Œå› ä¸ºæ˜¯æ²¡è§è¿‡çš„æ•°æ®ï¼‰
- å¦‚æœéªŒè¯æŸå¤±å¼€å§‹ä¸Šå‡ï¼Œè¯´æ˜è¿‡æ‹Ÿåˆäº†

---

## 19.9 ä¿å­˜æ¨¡å‹

### 19.9.1 ä¿å­˜æ£€æŸ¥ç‚¹

```python
# ä¿å­˜æ¨¡å‹
import os

if not os.path.exists('model/'):
    os.makedirs('model/')

torch.save({
    'model_state_dict': model.state_dict(),
    'h_params': h_params
}, 'model/model.ckpt')

print("æ¨¡å‹å·²ä¿å­˜åˆ° model/model.ckpt")
```

### 19.9.2 ä¿å­˜ä»€ä¹ˆï¼Ÿ

| å†…å®¹ | ä½œç”¨ |
|-----|------|
| `model.state_dict()` | æ‰€æœ‰æ¨¡å‹å‚æ•° |
| `h_params` | è¶…å‚æ•°ï¼ˆåŠ è½½æ—¶éœ€è¦ï¼‰ |

ä¿å­˜è¶…å‚æ•°æ˜¯ä¸ºäº†ä¹‹ååŠ è½½æ—¶èƒ½ç”¨**ç›¸åŒçš„é…ç½®**é‡å»ºæ¨¡å‹ã€‚

---

## 19.10 å®Œæ•´ train.py ä»£ç 

```python
"""
Train a Transformer model
"""
import os
import torch
import tiktoken
from model import Model

# GPU å†…å­˜é…ç½®
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"
torch.cuda.empty_cache()

# è¶…å‚æ•°
h_params = {
    "d_model": 80,
    "batch_size": 2,
    "context_length": 128,
    "num_blocks": 6,
    "num_heads": 4,
    "dropout": 0.1,
    "max_iters": 500,
    "learning_rate": 1e-3,
    "eval_interval": 50,
    "eval_iters": 10,
    "device": "cuda" if torch.cuda.is_available() else
              ("mps" if torch.backends.mps.is_available() else "cpu"),
    "TORCH_SEED": 1337
}
torch.manual_seed(h_params["TORCH_SEED"])

# åŠ è½½æ•°æ®
with open('data/è®¢å•å•†å“åç§°.csv', 'r', encoding="utf-8") as file:
    text = file.read()

# åˆ†è¯
tokenizer = tiktoken.get_encoding("cl100k_base")
tokenized_text = tokenizer.encode(text)
max_token_value = max(tokenized_text) + 1
h_params['max_token_value'] = max_token_value
tokenized_text = torch.tensor(tokenized_text, dtype=torch.long, device=h_params['device'])

print(f"Total: {len(tokenized_text):,} tokens")

# åˆ†å‰²æ•°æ®
train_size = int(len(tokenized_text) * 0.9)
train_data = tokenized_text[:train_size]
val_data = tokenized_text[train_size:]

# åˆå§‹åŒ–æ¨¡å‹
model = Model(h_params).to(h_params['device'])


def get_batch(split: str):
    data = train_data if split == 'train' else val_data
    idxs = torch.randint(low=0, high=len(data) - h_params['context_length'],
                         size=(h_params['batch_size'],))
    x = torch.stack([data[idx:idx + h_params['context_length']] for idx in idxs])
    y = torch.stack([data[idx + 1:idx + h_params['context_length'] + 1] for idx in idxs])
    return x.to(h_params['device']), y.to(h_params['device'])


@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'valid']:
        losses = torch.zeros(h_params['eval_iters'])
        for k in range(h_params['eval_iters']):
            x_batch, y_batch = get_batch(split)
            logits, loss = model(x_batch, y_batch)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out


# è®­ç»ƒå¾ªç¯
optimizer = torch.optim.AdamW(model.parameters(), lr=h_params['learning_rate'])

for step in range(h_params['max_iters']):
    if step % h_params['eval_interval'] == 0 or step == h_params['max_iters'] - 1:
        losses = estimate_loss()
        print(f'Step: {step}, Training Loss: {losses["train"]:.3f}, '
              f'Validation Loss: {losses["valid"]:.3f}')

    xb, yb = get_batch('train')
    logits, loss = model(xb, yb)

    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()

# ä¿å­˜æ¨¡å‹
if not os.path.exists('model/'):
    os.makedirs('model/')

torch.save({
    'model_state_dict': model.state_dict(),
    'h_params': h_params
}, 'model/model.ckpt')

print("Training complete. Model saved to model/model.ckpt")
```

---

## 19.11 å¯é€‰ï¼šWandB è®­ç»ƒè¿½è¸ª

### 19.11.1 ä»€ä¹ˆæ˜¯ WandBï¼Ÿ

[Weights & Biases](https://wandb.ai/) æ˜¯ä¸€ä¸ªè®­ç»ƒè¿½è¸ªå·¥å…·ï¼Œå¯ä»¥ï¼š
- å¯è§†åŒ–æŸå¤±æ›²çº¿
- è®°å½•è¶…å‚æ•°
- å¯¹æ¯”ä¸åŒå®éªŒ

### 19.11.2 é›†æˆä»£ç 

```python
# WandB é›†æˆï¼ˆå¯é€‰ï¼‰
import wandb

# åˆå§‹åŒ–
run = wandb.init(
    project="LLMZhang_lesson_2",
    config={
        "d_model": h_params["d_model"],
        "batch_size": h_params["batch_size"],
        "context_length": h_params["context_length"],
        "max_iters": h_params["max_iters"],
        "learning_rate": h_params["learning_rate"],
    },
)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­è®°å½•
for step in range(h_params['max_iters']):
    ...
    wandb.log({
        "train_loss": losses['train'].item(),
        "valid_loss": losses['valid'].item()
    })
```

---

## 19.12 æœ¬ç« æ€»ç»“

### 19.12.1 è®­ç»ƒæµç¨‹

```
1. åŠ è½½æ•°æ® â†’ åˆ†è¯ â†’ è½¬ Tensor â†’ åˆ†å‰² train/val

2. è®­ç»ƒå¾ªç¯ï¼š
   for step in range(max_iters):
       x, y = get_batch('train')     # è·å–æ•°æ®
       logits, loss = model(x, y)    # å‰å‘ä¼ æ’­
       optimizer.zero_grad()         # æ¸…é›¶æ¢¯åº¦
       loss.backward()               # åå‘ä¼ æ’­
       optimizer.step()              # æ›´æ–°å‚æ•°

3. ä¿å­˜æ¨¡å‹ â†’ torch.save()
```

### 19.12.2 å…³é”®å‡½æ•°

| å‡½æ•° | ä½œç”¨ |
|------|------|
| `get_batch()` | éšæœºè·å–ä¸€ä¸ª batch |
| `estimate_loss()` | åœ¨ train/val ä¸Šè¯„ä¼°æŸå¤± |
| `model.train()` | åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼ |
| `model.eval()` | åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ |
| `loss.backward()` | åå‘ä¼ æ’­ |
| `optimizer.step()` | æ›´æ–°å‚æ•° |

### 19.12.3 æ ¸å¿ƒè®¤çŸ¥

> **train.py ä¸åˆ° 100 è¡Œä»£ç ï¼Œä½†å®ç°äº†å®Œæ•´çš„è®­ç»ƒæµç¨‹ã€‚æ ¸å¿ƒå°±æ˜¯å››æ­¥å¾ªç¯ï¼šå‰å‘ä¼ æ’­ â†’ è®¡ç®—æŸå¤± â†’ åå‘ä¼ æ’­ â†’ æ›´æ–°å‚æ•°ã€‚PyTorch çš„è‡ªåŠ¨å¾®åˆ†è®©æˆ‘ä»¬åªéœ€è¦å®šä¹‰å‰å‘ä¼ æ’­ï¼Œåå‘ä¼ æ’­è‡ªåŠ¨å®Œæˆã€‚**

---

## æœ¬ç« äº¤ä»˜ç‰©

å­¦å®Œè¿™ä¸€ç« ï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] ç†è§£è®­ç»ƒå¾ªç¯çš„å››ä¸ªæ­¥éª¤
- [ ] çŸ¥é“ x å’Œ y çš„å…³ç³»ï¼ˆå³ç§»ä¸€ä½ï¼‰
- [ ] ç†è§£ `model.train()` å’Œ `model.eval()` çš„åŒºåˆ«
- [ ] èƒ½ç‹¬ç«‹å†™å‡ºä¸€ä¸ªç®€å•çš„è®­ç»ƒè„šæœ¬

---

## å®Œæ•´ä»£ç 

æœ¬ç« ä»£ç å¯¹åº”çš„å®Œæ•´å®ç°å¯åœ¨ GitHub è·å–ï¼š

> ğŸ“¦ **[github.com/waylandzhang/Transformer-from-scratch](https://github.com/waylandzhang/Transformer-from-scratch)**

åŒ…å« `model.py`ã€`train.py`ã€`inference.py` ä»¥åŠ step-by-step Jupyter notebookã€‚

---

## ä¸‹ä¸€ç« é¢„å‘Š

æ¨¡å‹è®­ç»ƒå¥½äº†ï¼Œå‚æ•°å·²ç»ä¿å­˜ã€‚ç°åœ¨æˆ‘ä»¬è¦ç”¨å®ƒæ¥**ç”Ÿæˆæ–‡æœ¬**ï¼

ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬æ¥å†™ **inference.py**ï¼šåŠ è½½æ¨¡å‹ã€è¾“å…¥ promptã€è®©æ¨¡å‹è‡ªå›å½’ç”Ÿæˆã€‚çœ‹çœ‹å®ƒå­¦åˆ°äº†ä»€ä¹ˆï¼
