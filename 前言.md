# 前言

## 为什么写这本书

2023 年底，我在 Bilibili 上发布了一系列讲解 Transformer 的视频。最初只是想把自己学习的过程记录下来，没想到收到了很多观众的反馈："张老师，能不能出个文字版？视频看完就忘了，想要能反复查阅的资料。"

这本书就是那个"文字版"。

但它不是视频的简单转录。在整理的过程中，我重新思考了每一个概念的讲解顺序，补充了大量视频中一笔带过的细节，修正了一些当时理解不够准确的地方，并加入了 2024-2025 年的最新进展——从 OpenAI 的 o1/o3 到 DeepSeek 的 R1，从 Mixture of Experts 到 Mamba 架构。

这些内容在我录制视频时还不存在。AI 领域的发展速度，确实超出了所有人的预期。

---

## 这本书的写作理念

**直觉优先，公式其次。**

我见过太多技术书籍，上来就是一堆公式，读者还没建立起任何直觉，就被符号淹没了。这本书的每一章都遵循同样的结构：

1. **先讲"为什么"** —— 这个东西要解决什么问题？
2. **再建立直觉** —— 用类比、图示、几何解释让你"感受"到它
3. **然后看公式** —— 有了直觉之后，公式只是精确的描述
4. **最后写代码** —— 能跑起来的代码，才是真正的理解

如果你读完一章，能用自己的话向别人解释清楚，而不是只会背公式——那这一章就成功了。

---

## 谁适合读这本书

**这本书适合你，如果：**

- 你用过 ChatGPT，想知道它背后是怎么工作的
- 你看过一些 Transformer 的介绍，但总觉得似懂非懂
- 你想从零实现一个 GPT，而不是只会调 API
- 你是算法工程师，需要一本能快速查阅的中文参考书
- 你想跟上 2024-2025 年的 AI 前沿进展

**这本书可能不适合你，如果：**

- 你是深度学习的完全新手（建议先学习基础的神经网络知识）
- 你需要严格的数学证明（这不是一本学术专著）
- 你只想快速调用现成的模型（直接用 Hugging Face 就好）

---

## 如何阅读这本书

**快速入门（1-2 天）：**
Part 1 全部 → 第 10 章（QKV）→ 第 15 章（完整前向传播）

**系统学习（1-2 周）：**
Part 1-5 顺序阅读，跟着敲代码

**生产部署（按需）：**
Part 6-8，重点关注 Flash Attention、KV Cache、量化

**前沿追踪（2024-2025）：**
Part 9，了解 RLHF、MoE、推理模型、后 Transformer 架构

每章结尾都有"本章交付物"清单，你可以用它检验自己是否真正理解了。

---

## 关于代码

本书的代码都可以实际运行。我选择从零手写而不是调用框架，是因为：

```python
# 这行代码：
output = nn.MultiheadAttention(embed_dim, num_heads)(query, key, value)

# 不如这样写更能帮助理解：
scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
scores = scores.masked_fill(mask == 0, -1e9)
attention_weights = F.softmax(scores, dim=-1)
output = torch.matmul(attention_weights, V)
```

当你能写出第二种代码时，你才真正理解了 Attention。

---

## 致谢

感谢所有在 Bilibili 上留言、提问、纠错的观众。你们的反馈让我意识到哪些地方讲得不够清楚，哪些地方需要补充。这本书的很多改进，都来自于你们的问题。

感谢 Geoffrey Hinton、Ilya Sutskever、Andrej Karpathy 等前辈的公开课程和分享。站在巨人的肩膀上，我们才能看得更远。

感谢我的家人，容忍我在深夜和周末对着电脑自言自语地录视频、写文档。

最后，感谢你选择阅读这本书。希望它能帮助你真正理解 Transformer，而不只是"知道"它。

---

**Wayland Zhang（张老师）**

2024 年 4 月初稿，2025 年 1 月修订

*"The best way to learn is to teach."*

---

## 勘误与反馈

如果你发现书中的错误，或有任何建议，欢迎通过以下方式联系我：

- Bilibili: @LLM张老师
- GitHub: github.com/WaylandZhang

我会持续更新和修正内容。技术书籍难免有疏漏，感谢你的理解和帮助。
